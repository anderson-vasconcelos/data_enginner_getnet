{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Desafio Engenheiro Getnet .ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMVvD7dd32wPLmoHKR8VJg8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anderson-vasconcelos/data_enginner_getnet/blob/main/Desafio_Engenheiro_Getnet_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Clf2Zr0b4MyZ"
      },
      "source": [
        "# instalar as dependências\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTAOIk4F4cim"
      },
      "source": [
        "# configurar as variáveis de ambiente\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\"\n",
        "\n",
        "# tornar o pyspark \"importável\"\n",
        "import findspark\n",
        "findspark.init('spark-2.4.4-bin-hadoop2.7')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShGvirfU4gEK",
        "outputId": "f323566f-320d-4ca1-93f2-c25ba518cb1e"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "transacoes = [{'trasaction_id': 1, 'client_id': 3545, 'total_amount': 3000, 'discount_percentage': 6.99},\n",
        "              {'trasaction_id': 2, 'client_id': 3545, 'total_amount': 4500, 'discount_percentage': 0.45},\n",
        "              {'trasaction_id': 3, 'client_id': 3509, 'total_amount': 69998, 'discount_percentage': 0.00},\n",
        "              {'trasaction_id': 4, 'client_id': 3510, 'total_amount': 1, 'discount_percentage': None},\n",
        "              {'trasaction_id': 5, 'client_id': 4510, 'total_amount': 34, 'discount_percentage': 40.00}]\n",
        "contratos = [{'contratct_id': 3, 'client_id': 3545, 'client_name': 'Magazine Luana', 'percentage': 2.00, 'is_active': True},\n",
        "             {'contratct_id': 4, 'client_id': 3545, 'client_name': 'Magazine Luana', 'percentage': 1.95, 'is_active': False},\n",
        "             {'contratct_id': 5, 'client_id': 3509, 'client_name': 'Lojas Italianas', 'percentage': 1.00, 'is_active': True},\n",
        "             {'contratct_id': 6, 'client_id': 3510, 'client_name': 'Carrerfive', 'percentage': 3.00, 'is_active': True}]\n",
        "\n",
        "sc = SparkSession.builder.master('local[*]').getOrCreate()\n",
        "\n",
        "df_transacoes = sc.createDataFrame(transacoes)\n",
        "df_contratos = sc.createDataFrame(contratos)\n",
        "\n",
        "df_transacoes.fillna(0,[\"discount_percentage\"]) \\\n",
        "  .join(df_contratos, df_transacoes[\"client_id\"] ==  df_contratos[\"client_id\"], \"inner\") \\\n",
        "  .filter(F.col('is_active') == True) \\\n",
        "  .withColumn('total_amount_percentage', F.col('total_amount') * (F.col('percentage') / 100.00)) \\\n",
        "  .withColumn('total_amount_percentage_discount_percentage', F.col(\"total_amount_percentage\") * (F.col('discount_percentage') / 100.00)) \\\n",
        "  .withColumn('total_gain', F.col('total_amount_percentage') - F.col('total_amount_percentage_discount_percentage')) \\\n",
        "  .agg(F.sum(\"total_gain\")) \\\n",
        "  .select(F.round(F.col(\"sum(total_gain)\"),3).alias(\"total_gain\")) \\\n",
        "  .show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "spark-2.4.4-bin-hadoop2.7/python/pyspark/sql/session.py:346: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
            "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+----------+\n",
            "|total_gain|\n",
            "+----------+\n",
            "|   845.411|\n",
            "+----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2OiqN8e41K_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}